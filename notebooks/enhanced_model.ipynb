{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo otimizado\n",
    "\n",
    "O código comentado explicando cada etapa está em \"basic_model.ipynb\"\n",
    "\n",
    "Nessa etapa focamos em comentar apenas as otimizações realizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator # type: ignore\n",
    "\n",
    "IMG_HEIGHT = 150\n",
    "IMG_WIDTH = 150\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "TRAIN_DIR = '../processed_data/dataset_1_split/train'\n",
    "VALIDATION_DIR = '../processed_data/dataset_1_split/validation'\n",
    "\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,             \n",
    "    rotation_range=40,          \n",
    "    width_shift_range=0.2,      \n",
    "    height_shift_range=0.2,     \n",
    "    shear_range=0.2,            \n",
    "    zoom_range=0.2,             \n",
    "    horizontal_flip=True,       \n",
    "    fill_mode='nearest'         \n",
    ")\n",
    "\n",
    "\n",
    "validation_datagen = ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ")\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True               \n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    VALIDATION_DIR,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False              \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models # type: ignore\n",
    "\n",
    "# --- ARQUITETURA DA CNN MELHORADA---\n",
    "model = models.Sequential(name='CNN_Alimentos_Parte1')\n",
    "\n",
    "# --- BLOCO 1 ---\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3), padding='same', name='conv1_1'))\n",
    "model.add(layers.MaxPooling2D((2, 2), name='pool1'))\n",
    "\n",
    "# --- BLOCO 2 ---\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same' ,name='conv2_1'))\n",
    "model.add(layers.MaxPooling2D((2, 2), name='pool2'))\n",
    "\n",
    "# --- BLOCO 3 ---\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same', name='conv3_1'))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same', name='conv3_2'))    #Camada extra\n",
    "model.add(layers.MaxPooling2D((2, 2), name='pool3'))\n",
    "\n",
    "# --- CLASSIFICADOR (MLP) ---\n",
    "model.add(layers.Flatten(name='flatten'))\n",
    "\n",
    "model.add(layers.Dense(256, activation='relu', name='dense1'))\n",
    "\n",
    "model.add(layers.Dropout(0.5, name='dropout'))\n",
    "\n",
    "model.add(layers.Dense(16, activation='softmax', name='output'))\n",
    "\n",
    "# --- FIM DA ARQUITETURA ---\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementando Callbacks para o treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau # type: ignore\n",
    "\n",
    "# 1. Salva o melhor modelo encontrado com base na menor perda de validação\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    filepath='modelo_otimizado.h5',  # Nome do arquivo para salvar o melhor modelo\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 2. Para o treinamento se não houver melhora na perda de validação por 5 épocas\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True, # Restaura os pesos da melhor época encontrada\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 3. Reduz a taxa de aprendizado se a perda de validação não melhorar por 5 épocas\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2, # Reduz o LR por um fator de 5 (1/5 = 0.2)\n",
    "    patience=5,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compilação e Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',                   # Otimizador adam utilizado\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=[                           # Novas métricas utilizadas para avaliação das epocas de treinamento\n",
    "        'accuracy', \n",
    "        tf.keras.metrics.Precision(name='precision'),\n",
    "        tf.keras.metrics.Recall(name='recall'),\n",
    "        tf.keras.metrics.AUC(name='auc')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100 # EarlyStopping vai cuidar do resto\n",
    "\n",
    "history_otimizado = model.fit(\n",
    "    train_generator,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=[model_checkpoint, early_stopping, reduce_lr] # Adiciona a lista de callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise do treinamento do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history_otimizado.history['accuracy']\n",
    "val_acc = history_otimizado.history['val_accuracy']\n",
    "loss = history_otimizado.history['loss']\n",
    "val_loss = history_otimizado.history['val_loss']\n",
    "\n",
    "epochs_range = range(len(acc))\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Acurácia de Treino')\n",
    "plt.plot(epochs_range, val_acc, label='Acurácia de Validação')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Acurácia de Treino vs. Validação')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Acurácia')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Perda de Treino')\n",
    "plt.plot(epochs_range, val_loss, label='Perda de Validação')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Perda de Treino vs. Validação')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Perda (Loss)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salvando o modelo otimizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva o modelo inteiro (arquitetura + pesos)\n",
    "model.save('modelo_otimizado.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
